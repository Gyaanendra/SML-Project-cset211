# ğŸ“° Fake News Classifier using RoBERTa + BiLSTM, BERT & Machine Learning Models

A comprehensive fake news classifier system that combines traditional Machine Learning models with advanced Transformer-based architectures (RoBERTa + BiLSTM) to classify news articles as **Real** or **Fake** with high accuracy.

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)](https://www.tensorflow.org/)
[![HuggingFace](https://img.shields.io/badge/ğŸ¤—-Transformers-yellow.svg)](https://huggingface.co/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## ğŸ“‹ Table of Contents

- [Problem Statement](#-problem-statement)
- [Project Overview](#-project-overview)
- [Dataset](#-dataset)
- [Directory Structure](#-directory-structure)
- [Installation](#-installation)
- [Preprocessing Pipeline](#-preprocessing-pipeline)
- [Model Architecture](#-model-architecture)
- [Technologies Used](#-technologies-used)
- [Results](#-results)
- [Deployment](#-deployment)
- [Usage](#-usage)
- [Future Enhancements](#-future-enhancements)
- [Contributors](#-contributors)

---

## ğŸ¯ Problem Statement

Fake news spreads rapidly across digital platforms, influencing public opinion and creating widespread misinformation. This project aims to **automatically detect fake news** using Natural Language Processing (NLP) and Deep Learning techniques to ensure reliable and accurate classification of news articles.

---

## ğŸš€ Project Overview

This system employs a **dual-approach strategy**:

### ğŸ”¹ Traditional Machine Learning Models

- **Logistic Regression**
- **Random Forest**
- **Naive Bayes**
- **XGBoost**

These models are trained using **TF-IDF (Term Frequency-Inverse Document Frequency)** features extracted from preprocessed text.

### ğŸ”¹ Transformer + LSTM Hybrid Models

- **RoBERTa + LSTM**
- **RoBERTa + BiLSTM** â­ **(Best Model)**
- **DistilBERT + LSTM**
- **DistilBERT + BiLSTM**

Transformers extract high-quality contextual embeddings, while LSTM/BiLSTM layers capture sequential patterns and dependencies in text.

### ğŸ† Best Model Performance

**RoBERTa + Bi-LSTM** achieved:

- **Accuracy**: 95.49%
- **ROC-AUC**: 0.9914

---

## ğŸ“Š Dataset

The project uses a merged dataset from two reliable public sources:

### 1. **WELFake Dataset**

Contains labeled fake and real news articles with comprehensive metadata.

### 2. **ISOT Dataset**

Provides two separate CSV files:

- `Fake.csv` - Fake news articles
- `True.csv` - Real news articles

### Dataset Statistics

- **Total Articles**: 62,200
- **Features**: Title + Text (combined)
- **Preprocessing**: Cleaned, deduplicated, and normalized
- **Labels**: Binary (0 = Fake, 1 = Real)

---

## ğŸ”„ Pipeline Architecture

<div align="center">

![Fake News Classifier Pipeline](./mermaid-diagram-2025-11-20-221651.png)

</div>

---

## ğŸ“ Directory Structure

```
gyaanendra-sml-project-cset211/
â”‚
â”œâ”€â”€ README.md                          # Project documentation
â”œâ”€â”€ LICENSE                            # License information
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”‚
â”œâ”€â”€ raw_data/                          # Original datasets
â”‚   â”œâ”€â”€ Fake.csv                       # ISOT fake news
â”‚   â”œâ”€â”€ True.csv                       # ISOT real news
â”‚   â””â”€â”€ WELFake_Dataset.csv            # WELFake dataset
â”‚
â”œâ”€â”€ merged_data/                       # Combined dataset
â”‚   â””â”€â”€ data.csv                       # Merged raw data
â”‚
â”œâ”€â”€ cleaned_preprocessed_data/         # Preprocessed data
â”‚   â””â”€â”€ data_textCleanedProcessed.csv  # Cleaned & processed text
â”‚
â””â”€â”€ streamlit/                         # Deployment application
    â”œâ”€â”€ app.py                         # Streamlit web app
    â”œâ”€â”€ requirements.txt               # App dependencies
    â”œâ”€â”€ test.txt                       # Sample test cases
    â””â”€â”€ best_tokenizer/                # Saved tokenizer
        â”œâ”€â”€ special_tokens_map.json
        â””â”€â”€ tokenizer_config.json
```

---

## ğŸ”§ Installation

### Prerequisites

- Python 3.12.11
- pip package manager

### Setup

1. **Clone the repository**

```bash
git clone https://github.com/yourusername/gyaanendra-sml-project-cset211.git
cd gyaanendra-sml-project-cset211
```

2. **Install dependencies**

```bash
pip install -r requirements.txt
```

3. **For Streamlit deployment**

```bash
cd streamlit
pip install -r requirements.txt
```

---

## ğŸ§¹ Preprocessing Pipeline

The preprocessing pipeline ensures clean, normalized text for optimal model performance:

1. **Text Lowercasing** - Convert all text to lowercase
2. **URL Removal** - Remove hyperlinks and web addresses
3. **Punctuation & Number Removal** - Clean special characters and digits
4. **Stopword Removal** - Remove common words using NLTK
5. **Lemmatization** - Reduce words to their base form
6. **Feature Combination** - Merge `title` + `text` fields
7. **Deduplication** - Remove duplicate articles

### Output Formats

- **For ML Models**: TF-IDF vectors
- **For Transformers**: Tokenized input_ids + attention_masks

---

## ğŸ—ï¸ Model Architecture

### Traditional ML Pipeline

```
Cleaned Text â†’ TF-IDF Vectorization â†’ ML Model â†’ Classification (Real/Fake)
```

### Transformer + BiLSTM Pipeline

```
Input Text
    â†“
Tokenizer (input_ids, attention_mask)
    â†“
RoBERTa/DistilBERT Encoder (Frozen)
    â†“
Sequence Output: (batch, 128, 768)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         BiLSTM Layer                â”‚
â”‚  Forward:  Token1 â†’ Token2 â†’ ... â†’  â”‚
â”‚  Backward: ... â† Token2 â† Token1    â”‚
â”‚  Output: [forward_h + backward_h]   â”‚
â”‚          â†’ (batch, 256)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Dropout(0.3)
    â†“
Dense(64, ReLU)
    â†“
Dense(1, Sigmoid) â†’ Probability [0,1]
```

### Key Architecture Features

- **Contextual Embeddings**: Transformers capture semantic meaning
- **Sequential Learning**: BiLSTM processes text bidirectionally
- **Regularization**: Dropout prevents overfitting
- **Binary Classification**: Sigmoid activation for probability output

---

## ğŸ› ï¸ Technologies Used

| Category             | Technologies                                   |
| -------------------- | ---------------------------------------------- |
| **Programming**      | Python 3.12.11                                 |
| **Data Processing**  | Pandas, NumPy                                  |
| **NLP**              | NLTK (Stopwords, Lemmatization)                |
| **Machine Learning** | Scikit-Learn, XGBoost                          |
| **Deep Learning**    | TensorFlow, Keras                              |
| **Transformers**     | HuggingFace Transformers (RoBERTa, DistilBERT) |
| **Visualization**    | Matplotlib, Seaborn                            |
| **Deployment**       | Streamlit                                      |

---

## ğŸ“ˆ Results

### Model Comparison

| Model                | Accuracy   | ROC-AUC    | Notes               |
| -------------------- | ---------- | ---------- | ------------------- |
| Naive Bayes          | 84.10%     | -          | Baseline            |
| Random Forest        | 93.82%     | -          | Good generalization |
| Logistic Regression  | 94.46%     | -          | Strong baseline     |
| XGBoost              | 96.05%     | -          | Best traditional ML |
| RoBERTa + LSTM       | 94.80%     | 0.9875     | Unidirectional      |
| **RoBERTa + BiLSTM** | **95.49%** | **0.9914** | **Best Overall** â­ |
| DistilBERT + BiLSTM  | 94.92%     | 0.9881     | Faster inference    |

### Key Insights

- **Hybrid models** outperform traditional ML approaches
- **BiLSTM** captures bidirectional context better than LSTM
- **RoBERTa embeddings** provide superior semantic understanding
- **XGBoost** is the best traditional ML model

---

## ğŸš€ Deployment

### Files Included

- **Saved Model**: `best_model.h5` (RoBERTa + BiLSTM)
- **Tokenizer**: Saved in `best_tokenizer/` directory
- **Streamlit App**: Interactive web interface

### Run the Streamlit App

```bash
cd streamlit
streamlit run app.py
```

### Prediction Function Example

```python
from transformers import RobertaTokenizer
from tensorflow.keras.models import load_model

# Load model and tokenizer
model = load_model('best_model.h5')
tokenizer = RobertaTokenizer.from_pretrained('./best_tokenizer')

def predict(text):
    # Tokenize input
    inputs = tokenizer(text, max_length=128, padding='max_length',
                      truncation=True, return_tensors='tf')

    # Get prediction
    prediction = model.predict([inputs['input_ids'], inputs['attention_mask']])

    return "REAL" if prediction[0][0] > 0.5 else "FAKE"

# Test
result = predict("Breaking news: Scientists discover new planet!")
print(f"Prediction: {result}")
```

## ğŸ”® Future Enhancements

- [ ] **Multilingual Support** - Detect fake news in multiple languages
- [ ] **Real-time Monitoring** - Integration with social media APIs
- [ ] **Explainable AI** - LIME/SHAP for model interpretability
- [ ] **Browser Extension** - Chrome extension for instant verification
- [ ] **Domain-specific Fine-tuning** - Specialized models for health, politics, finance
- [ ] **Fact-checking Integration** - Link with fact-checking databases
- [ ] **Mobile Application** - Android/iOS app deployment

---

## ğŸ‘¥ Contributors

- **Gyanendra Prakash**
  - **GitHub**: [@Gyaanendra](https://github.com/Gyaanendra)
- **Goutam Mittal**
  - **GitHub**: [@goutam922](https://github.com/goutam922)
- **Dhruv Gupta**
  - **GitHub**: [@dhruvv1402](https://github.com/dhruvv1402)

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **WELFake Dataset** creators for providing quality data
- **ISOT Research Lab** for the fake news dataset
- **HuggingFace** for Transformer models
- **TensorFlow Team** for deep learning framework

---

## ğŸ“§ Contact

For questions or collaborations:

- **Email**: gyaanendrap@gmail.com
- **GitHub**: [@Gyaanendra](https://github.com/Gyaanendra)

---

<div align="center">

### â­ If you find this project useful, please consider giving it a star!

**Made with â¤ï¸ for fighting misinformation**

</div>
